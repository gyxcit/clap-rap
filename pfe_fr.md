# Titre  : 
CLAP-RAP-FR : ModÃ¨le Multimodal Modulaire pour lâ€™Analyse du Rap FranÃ§ais et la Recommandation Musicale Ã‰motionnelle

# RÃ©sumÃ©
Ce Projet de Fin dâ€™Ã‰tudes vise Ã  dÃ©velopper un modÃ¨le multimodal innovant basÃ© sur lâ€™architecture CLAP (Contrastive Languageâ€“Audio Pretraining), entiÃ¨rement spÃ©cialisÃ© pour le rap franÃ§ais. Lâ€™objectif est de crÃ©er une base dâ€™intelligence artificielle capable de comprendre le contenu audio, le style, les paroles et les Ã©motions propres au rap francophone, puis de construire un systÃ¨me de recommandation musicale basÃ© sur les Ã©motions ressenties par lâ€™utilisateur.

Le projet explore Ã©galement une extension moderne et modulaire du modÃ¨le via lâ€™intÃ©gration de techniques avancÃ©es de LoRA (Low-Rank Adaptation) et de Mixture-of-Experts (MoE) afin de permettre lâ€™ajout illimitÃ© de nouvelles compÃ©tences au modÃ¨le (nouveaux sous-genres, Ã©motions, styles, artistes), sans nÃ©cessiter de rÃ©entraÃ®ner lâ€™ensemble de lâ€™architecture.

# ğŸ¯ Objectifs

Construire un modÃ¨le CLAP spÃ©cialisÃ© pour le rap franÃ§ais, capable dâ€™apprendre simultanÃ©ment :

- la structure audio (flow, rythmique, productionâ€¦)

- les paroles et leur sÃ©mantique

- les Ã©motions et thÃ¨mes du rap franÃ§ais

Constituer un dataset multimodal inÃ©dit :

- extraits audio de rap FR

- paroles (lyrics)

- annotations Ã©motionnelles (mÃ©lancolie, Ã©gotrip, storytelling, rage, chillâ€¦)

- sous-genres (drill FR, trap, cloud, old-school, afro-trap, etc.)

Mettre en place une architecture modulaire basÃ©e sur :

- un CLAP-Core gelÃ© qui joue le rÃ´le de base gÃ©nÃ©rale

- des LoRA adapters pour ajouter facilement de nouvelles connaissances

- une couche Mixture-of-Experts (MoE) pour spÃ©cialiser automatiquement le modÃ¨le selon le sous-genre ou le type dâ€™Ã©motion

Concevoir un systÃ¨me de recommandation musicale Ã©motionnelle :

- lâ€™utilisateur exprime un Ã©tat Ã©motionnel (â€œmotivationâ€, â€œmÃ©lancolieâ€, â€œrageâ€, â€œchillâ€)

- le systÃ¨me gÃ©nÃ¨re les morceaux les plus pertinents dans lâ€™espace embedding du CLAP-RAP-FR

- Ã‰valuer scientifiquement le modÃ¨le :

- retrieval audio â†” texte â†” Ã©motion

- classification dâ€™Ã©motions

- reconnaissance de sous-genres

- Ã©tude utilisateur pour valider la qualitÃ© de la recommandation

ğŸ§  Innovation

Le projet est scientifiquement original car :

- Aucun CLAP nâ€™est spÃ©cialisÃ© pour le rap franÃ§ais.

- Lâ€™intÃ©gration de LoRA + MoE dans un modÃ¨le multimodal audio/texte est nouvelle dans ce domaine.

- Le systÃ¨me de recommandation Ã©motionnelle basÃ© sur un espace CLAP spÃ©cialisÃ© rap FR est inÃ©dit.

- Le modÃ¨le devient extensible Ã  lâ€™infini sans rÃ©entraÃ®nement global.

Ce PFE se situe Ã  lâ€™intersection du deep learning multimodal, de la music information retrieval (MIR) et du traitement audio moderne, tout en sâ€™adressant Ã  un domaine culturel spÃ©cifique.

# ğŸ”§ MÃ©thodologie (rÃ©sumÃ©)

- Extraction dâ€™un dataset audio (20â€“30 secondes/morceau)

- RÃ©cupÃ©ration de paroles via APIs

- Annotation des Ã©motions

- EntraÃ®nement du CLAP-RAP-FR

- CongÃ©lation du modÃ¨le de base

- Ajout d'experts MoE pour diffÃ©rents styles et Ã©motions

- Ajout de LoRA pour affiner certains comportements

- Construction de lâ€™index vectoriel FAISS pour la recommandation

- Ã‰valuation et validation

# ğŸ“Œ Livrables

- ModÃ¨le CLAP-RAP-FR prÃ©entraÃ®nÃ©

- Module MoE spÃ©cialisÃ© sous-genres & Ã©motions

- Adapters LoRA modulaires

- Dataset multimodal rap FR annotÃ©

- SystÃ¨me complet de recommandation Ã©motionnelle

- Rapport + dÃ©monstration

# ğŸš€ Impact

Ce projet ouvre la voie Ã  :

- des systÃ¨mes de recommandation plus sensibles Ã  la culture musicale locale,

- des modÃ¨les audio/texte modulaires facilement extensibles,

- de nouvelles applications dans la production musicale, le streaming et lâ€™analyse automatique du rap.

